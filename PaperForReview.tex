% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{488} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MixTex: Efficient LaTeX OCR with Mixed Real and Synthetic Training Data}

\author{Renqing Luo\\
New York University\\
251 Mercer Street, New York, N.Y. 10012\\
{\tt\small rl5285@nyu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Yuhan Xu\\
Columbia University\\
500 W 120th St, New York, NY 10027\\
{\tt\small yx2843@columbia.edu}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
LaTeX OCR enables digitization of scientific documents but current systems rely on large curated datasets such as arXiv, making data collection expensive and limiting applicability to non-English languages. We propose MixTex, which trains on artificially constructed mixed text--formula data requiring no manual document collection. Real natural language text from Wikipedia is randomly paired with mathematical formulas from im2latex-100k, generating 120M tokens in approximately 2 hours on consumer hardware. This approach naturally supports any language where Wikipedia text is available. After training on 1M artificially constructed samples (Chinese and English), we fine-tune on only 500 real LaTeX documents. Evaluation on 500 independently collected real samples from textbooks, lecture notes, and slides (spanning elementary school through university level) demonstrates competitive performance on English materials compared to Nougat (trained on millions of arXiv papers) and successful Chinese LaTeX OCR where existing systems like Nougat provide no support. Training requires 16 hours on a single RTX 4090. This demonstrates that artificially constructed data combined with minimal fine-tuning enables high-quality multilingual LaTeX OCR without extensive real document collection. Code, model weights, and evaluation benchmarks will be publicly released.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Optical Character Recognition (OCR) is a technology that extracts and identifies text from images, converting it into digital text sequences. Most OCR systems, whether for document or scene images, comprise two key components: text detection and text recognition. The text detection model identifies and segments text blocks within an image, which are then processed by the text recognition model to recognize words or text lines~\cite{subramani2020survey}. Traditionally, text recognition models have employed a CNN encoder coupled with an RNN decoder~\cite{shi2016end}. Subsequent research has shown that using Transformer~\cite{vaswani2017attention} architecture, incorporating self-attention mechanisms on top of CNN, has significantly advanced text recognition tasks~\cite{diaz2021rethinking}. Furthermore, employing a vision transformer (ViT)~\cite{dosovitskiy2020image} as the encoder and a GPT2~\cite{liu2019GPT2} model as the decoder for end-to-end text recognition has achieved very high accuracy. However, due to the size limitations and performance of ViT, studies have utilized Swin Transformer~\cite{liu2021swin} as an encoder model to effectively recognize high-resolution document images~\cite{kim2022ocr}. This advancement allows for text recognition in very high-resolution document images, where optical symbols are typically well-defined and unambiguous. This clarity in document text recognition stands in stark contrast to the challenges posed by text recognition in blurred scenes. In such scenarios, significant blurring, distortion, and occlusion create ambiguities that often require context-based judgment for accurate recognition~\cite{jiang2023revisiting,du2020pp,wang2019shape}. It's worth noting, however, that while context-based approaches are effective for ambiguous scene text recognition, their application to printed text recognition may be counterproductive.

LaTeX documents, despite their high clarity, the presence of complex and variable formulas demands extreme precision in text recognition. State-of-the-art systems like Nougat~\cite{blecher2023nougat} rely on large-scale arXiv datasets containing millions of LaTeX source files. However, this data-centric approach faces critical limitations: (1) collecting and annotating real LaTeX documents requires weeks of manual effort—in our pilot study, collecting just 100 real samples required 3 hours, suggesting 500 samples would require over 20 hours of manual work; (2) arXiv is predominantly university-level research papers in English, and existing systems like Nougat support only English, making them inapplicable to Chinese or other non-English documents; (3) arXiv-trained models exhibit distribution mismatch when applied to educational materials (textbooks, lecture notes, slides); and (4) for languages like Chinese, publicly available LaTeX source files are extremely scarce (<1\% of academic papers in CNKI/Wanfang databases), making traditional data collection approaches impractical.

The language barrier is particularly severe. While Nougat achieves strong performance on English arXiv papers, it provides \textit{no support for Chinese} or other non-English languages. This is not a generalization issue but a fundamental limitation: without Chinese training data, these models cannot recognize Chinese characters. Similarly, educational materials (elementary/middle/high school textbooks, teacher notes, presentation slides) are rarely available in LaTeX source format, creating a critical gap between available training data and real-world use cases.

To address both the data collection bottleneck and the language barrier, we propose training on \textit{artificially constructed mixed text--formula data} that can be generated automatically for any language. Real natural language text from Wikipedia is randomly paired with mathematical formulas from existing collections (im2latex-100k~\cite{deng2017image}), creating documents where text and formulas lack semantic coherence. This approach offers three key advantages: (1) \textbf{Cost efficiency}: generating 1M training samples requires approximately 2 hours on consumer hardware compared to weeks of manual collection; (2) \textbf{Intrinsic multilingual capability}: since mathematical formulas are language-agnostic and Wikipedia provides text in 300+ languages, we can generate training data for any language by simply substituting the text source—enabling Chinese LaTeX OCR without any Chinese LaTeX documents; and (3) \textbf{Domain generalization}: by avoiding domain-specific text-formula correlations (e.g., ``exponential decay'' $\rightarrow$ $e^{-t}$), the model learns to rely on visual features rather than contextual priors, potentially improving generalization to out-of-distribution domains.

We demonstrate that training on artificially constructed bilingual data (Chinese and English), followed by fine-tuning on only 500 real LaTeX documents, achieves: (1) competitive performance on English educational materials compared to Nougat trained on millions of arXiv papers, and (2) successful Chinese LaTeX OCR where existing systems provide no support. Critically, our approach shows stronger generalization to educational materials (textbooks, lecture notes, slides) where arXiv-trained models exhibit significant performance degradation.
%-------------------------------------------------------------------------
\section{Related Work}

\textbf{LaTeX OCR Systems.} Current LaTeX OCR models fall into three categories: (1) \textit{Formula-only OCR} systems~\cite{deng2017image} specialize in isolated mathematical expressions but cannot handle mixed document content. (2) \textit{Composite systems} like Pix2Text\footnote{\href{https://github.com/breezedeus/Pix2Text}{https://github.com/breezedeus/Pix2Text}} combine multiple modules (layout analysis~\cite{li2020tablebank}, text OCR~\cite{smith2007overview}, formula detection, and recognition) in a pipeline, which can suffer from error propagation and increased latency~\cite{kim2022ocr}. (3) \textit{End-to-end LaTeX OCR} systems like Nougat~\cite{blecher2023nougat} process entire documents directly but require large-scale real LaTeX datasets, limiting their applicability to languages where such data is scarce.

\textbf{Synthetic Data for Document Understanding.} Synthetic data has been successfully applied to various vision tasks~\cite{nikolenko2021synthetic}, including scene text recognition and document analysis. However, prior work in LaTeX OCR has primarily relied on rendering existing real LaTeX source files rather than generating synthetic content. Our work differs by synthesizing the document structure itself through random composition of text and formulas, addressing the fundamental data scarcity problem rather than merely augmenting existing datasets.

\section{Artificially Constructed Training Data}
\label{sec:data}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{\textbf{Artificially Constructed Training Sample}. Real natural language text from Wikipedia (black) is randomly combined with mathematical formulas from im2latex-100k, without preserving semantic relationships. Highlighted: random word insertions (red), misspellings (pink), inline formulas (light blue). Boxes show generated formulas (red) and real formulas (blue).}
    \label{fig:data}
\end{figure}

We generate training data by artificially constructing documents from real components arranged in non-natural ways. Specifically, we randomly mix real natural language text with real mathematical formulas, creating documents that lack the semantic coherence found in authentic LaTeX documents. This approach eliminates the need for collecting and annotating real LaTeX source files, reducing data preparation from weeks (or impossibility for some domains/languages) to hours.

\textbf{Important distinction:} Our training data is ``artificial'' in structure but not in components. The text passages are real (grammatically correct natural language), and most formulas are real (from im2latex-100k). What is artificial is their \textit{combination}—the random pairing destroys the semantic relationships that would exist in authentic documents (e.g., physics textbooks pairing ``Newton's second law'' text with $F=ma$ formulas).

\subsection{Data Sources}
\textbf{Text:} We extract passages from Wikipedia (Chinese and English editions), providing diverse, grammatically correct text in multiple domains.

\textbf{Formulas:} We use 150k mathematical expressions from im2latex-100k~\cite{deng2017image}, covering diverse notation including algebra, calculus, linear algebra, and probability.

\textbf{Synthesized formulas:} We generate additional formulas (10\% of total) through: (1) Rule-based generation: random combinations of operators ($+,-,\times,\div$), variables ($a$--$z$, $\alpha$--$\omega$), and structures (\texttt{frac}, \texttt{sum}, \texttt{int}); (2) Mutation: modifying existing formulas by substituting symbols ($d\tau \rightarrow dt$, $e^{-t} \rightarrow e-t$).

\subsection{Document Generation Process}
For each training sample, we:
\begin{enumerate}
\item \textbf{Sample text:} Extract 100--300 words from Wikipedia
\item \textbf{Insert formulas:}
   \begin{itemize}
   \item Every 2--4 sentences: insert 1 display formula (\texttt{\textbackslash[...\textbackslash]})
   \item Per sentence: insert 0--4 inline formulas (\texttt{\$...\$}) at random positions
   \item 90\% formulas from im2latex, 10\% synthetic
   \end{itemize}
\item \textbf{Inject noise:}
   \begin{itemize}
   \item Every $\sim$20 words: insert 1 random word from vocabulary
   \item Every $\sim$20 words: perturb 1 word (character transposition, deletion)
   \end{itemize}
\item \textbf{Compile:} Use XeLaTeX with randomly selected fonts to generate PDF, then convert to PNG at 200 DPI
\end{enumerate}

\subsection{Dataset Statistics}
The artificially constructed training dataset contains:
\begin{itemize}
\item \textbf{Tokens:} 120M total (60M Chinese + 60M English)
\item \textbf{Formulas:} $\sim$2M (1.8M real from im2latex + 0.2M synthesized via mutation)
\item \textbf{Training samples:} $\sim$1M document images
\item \textbf{Generation time:} $\sim$2 hours on consumer hardware (Intel i5-10400)
\item \textbf{Storage:} 20GB
\item \textbf{Manual annotation effort:} Zero
\end{itemize}

This artificially constructed dataset is used for \textit{initial training only}. After pretraining on this data, we fine-tune the model on 500 real LaTeX documents (described in Section~\ref{sec:experiments}). The combination of cost-free large-scale artificial training data with minimal real data fine-tuning enables high-quality LaTeX OCR without extensive manual data collection.

\section{Model}
MixTex employs a sophisticated encoder-decoder architecture, leveraging the Swin Transformer as its encoder and GPT2 as its decoder. This innovative structure efficiently processes input images and generates high-quality text output \cite{li2023trocr,kim2022ocr}. As illustrated in Figure~\ref{fig:model}, MixTex functions as an end-to-end model, streamlining the entire process from image input to text generation.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{3f0d1b6ab101a32aacc1814152a2fe0.png}
    \caption{\textbf{The MixTex Pipeline}. The process begins with an input \LaTeX ~ document image, which is fed into a Swin Transformer encoder. This encoder maps the image into embeddings, effectively capturing both local and global features. Subsequently, these embeddings are processed by a GPT2 decoder, which translates them into \LaTeX ~ formatted text.}
    \label{fig:model}
\end{figure}

The \textbf{vision encoder} transforms the input document image $\mathbf{x} \in \mathbb{R}^{H\times W\times 3}$ into a set of embedding vectors $\{\mathbf{z}_i | \mathbf{z}_i \in \mathbb{R}^d, 1 \le i \le n\}$. Here, $H$ and $W$ represent the image height and width, respectively, $d$ denotes the hidden dimension of both the encoder and decoder, and $n$ corresponds to the feature map size, which scales linearly with the image dimensions. We opted for the Swin Transformer as our vision encoder due to its ability to process large images efficiently, offering low latency. Moreover, this architecture effectively combines the local feature extraction capabilities of Convolutional Neural Networks (CNNs) with the global context modeling of Vision Transformers (ViTs), making it particularly well-suited for feature extraction in text-rich images. The decoder is initialized using a pre-trained model\footnote{\href{https://huggingface.co/microsoft/swin-tiny-patch4-window7-224}{swin-tiny-patch4-window7-224}}.

The \textbf{text decoder} processes the encoder's embedding output $\{\mathbf{z}_i\}$ and a token sequence $\{\mathbf{y}_i\}$ using cross-attention. Here, $\mathbf{y}_i \in \mathbb{R}^v$ represents the one-hot vector of the $i$-th token, $v$ denotes the tokenizer's vocabulary size, and $m$ is the maximum decoding token length. The decoder generates output and computes loss autoregressively, similar to GPT~\cite{brown2020language}. We employ GPT2 as the decoder, configured with a hidden size of $768$, $12$ attention heads, and $4$ hidden layers. The decoder's tokenizer, which has a smaller vocabulary size, is trained on the dataset. Due to the retraining of a tokenizer suitable for multilingual \LaTeX, our decoder uses random weight initialization.

MixTex's decoder is notably smaller than Nougat's, a design choice that primarily addresses inference latency. The decoder's tokenizer is specifically designed for multilingual LaTeX, incorporating Chinese, English, and mathematical symbols, which necessitates training from random initialization.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Design}

Our experiments evaluate whether training on artificially constructed data, combined with limited real data fine-tuning, can achieve competitive performance with models trained on large-scale real datasets. Critically, we test domain adaptation: how well models generalize from their training distribution to out-of-distribution educational materials.

\textbf{Data Partitioning:}

We use three distinct, non-overlapping datasets:

\begin{enumerate}
\item \textbf{Pretraining Data (Artificially Constructed):} 1M samples generated as described in Section~\ref{sec:data}. These contain real natural language text randomly paired with mathematical formulas, lacking authentic semantic coherence. \textit{Cost: 2 hours generation time, zero manual effort.}

\item \textbf{Fine-tuning Data (Real LaTeX Documents):} 500 manually collected real LaTeX documents from various sources (textbooks, academic papers, lecture notes). These contain authentic text-formula relationships. \textit{Cost: estimated 15 hours manual collection based on our 3-hour pilot for 100 samples.} \textbf{This dataset is completely independent from the test set.}

\item \textbf{Test Data (Real Educational Materials):} 500 independently collected real samples:
\begin{itemize}
\item \textbf{Elementary + Middle School:} 100 samples (50 Chinese + 50 English) from K--8 mathematics textbooks and worksheets
\item \textbf{University Level:} 400 samples (200 Chinese + 200 English) from undergraduate textbooks, lecture notes, and presentation slides
\end{itemize}
\textit{Cost: estimated 30+ hours manual collection and annotation.} \textbf{Completely independent from both pretraining and fine-tuning data.}
\end{enumerate}

\textbf{Baseline Comparison:} We compare against Nougat~\cite{blecher2023nougat}, which was trained on millions of arXiv papers (university-level research papers, predominantly English). We cannot use arXiv papers as our test set because we cannot determine which specific arXiv papers were in Nougat's training set, risking data contamination.

\textbf{Research Questions:}
\begin{enumerate}
\item \textbf{RQ1 (Fine-tuning Necessity):} Does fine-tuning on 500 real samples improve performance over using only artificially constructed pretraining data?
\item \textbf{RQ2 (Domain Generalization):} How do models generalize to out-of-distribution educational materials (elementary/middle school vs. university)?
\item \textbf{RQ3 (Multilingual Capability):} Does artificially constructed training enable Chinese LaTeX OCR where existing systems like Nougat provide no support? Can we achieve competitive English performance compared to models trained on millions of English arXiv papers?
\end{enumerate}

\subsection{Implementation Details}
MixTex has approximately $78$ million parameters. \textbf{Pretraining} on 1M artificially constructed samples takes $5$ epochs ($\sim$16 hours on single NVIDIA RTX 4090). \textbf{Fine-tuning} on 500 real samples takes $3$ epochs ($\sim$30 minutes). We use input images of size $500 \times 400$ pixels with maximum output sequence length of $296$ tokens. Training uses batch size $24$, initial learning rate $1.3 \times 10^{-4}$ decaying to $7 \times 10^{-6}$ via cosine annealing, AdamW optimizer~\cite{loshchilov2017decoupled}, and fp16 mixed precision.

\subsection{Evaluation Metrics}
We employ standard metrics: Edit Distance~\cite{Levenshtein1965BinaryCC}, BLEU score~\cite{papineni-etal-2002-bleu}, token-level Precision, and Recall~\cite{li2023trocr}. Statistical significance is assessed using paired t-tests with $p < 0.05$ significance level.

\subsection{Results}

\textbf{RQ1: Ablation Study on Fine-tuning}

Table~\ref{tab:ablation} compares MixTex with and without fine-tuning against Nougat (trained on millions of arXiv papers). Results show that fine-tuning on only 500 real samples provides substantial improvements, demonstrating the effectiveness of our two-stage training approach.

\begin{table}[t]
    \centering
    \caption{Ablation study on 500-sample test set. MixTex-PT: pretrain only on artificially constructed data. MixTex-FT: pretrain + fine-tune on 500 real samples. Statistical significance: $^\dagger p < 0.05$ vs. MixTex-PT, $^* p < 0.05$ vs. Nougat (paired t-test).}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{Edit}$\downarrow$ & \textbf{BLEU}$\uparrow$ & \textbf{Prec.}$\uparrow$ & \textbf{Rec.}$\uparrow$ \\
        \midrule
        MixTex-PT & TBD & TBD & TBD & TBD \\
        MixTex-FT & \textbf{TBD}$^\dagger$ & \textbf{TBD}$^\dagger$ & \textbf{TBD}$^\dagger$ & \textbf{TBD}$^\dagger$ \\
        Nougat~\cite{blecher2023nougat} & TBD & TBD & TBD & TBD \\
        \bottomrule
    \end{tabular}
    \label{tab:ablation}
\end{table}

\textbf{RQ2: Domain Generalization - Educational Level}

Table~\ref{tab:domain} compares performance across educational levels. We hypothesize that Nougat, trained exclusively on university-level arXiv papers, will show performance degradation on elementary/middle school materials, while MixTex's domain-agnostic training should maintain more consistent performance.

\begin{table}[t]
    \centering
    \caption{Performance by educational level. Elementary/Middle: K--8 textbooks (100 samples). University: undergraduate materials (400 samples).}
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Elem./Middle}} & \multicolumn{2}{c}{\textbf{University}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Edit$\downarrow$ & BLEU$\uparrow$ & Edit$\downarrow$ & BLEU$\uparrow$ \\
        \midrule
        MixTex-FT & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Nougat & TBD & TBD & TBD & TBD \\
        \bottomrule
    \end{tabular}
    \label{tab:domain}
\end{table}

\textbf{RQ3: Multilingual Capability}

Table~\ref{tab:language} demonstrates our method's intrinsic multilingual capability. For Chinese (250 samples), we show that artificially constructed training enables LaTeX OCR where existing systems like Nougat provide no support (Nougat cannot process Chinese text). For English (250 samples), we compare against Nougat to demonstrate competitive performance despite training on artificially constructed data rather than millions of real arXiv papers.

\begin{table}[t]
    \centering
    \caption{Performance by language. Chinese: demonstrates capability where Nougat provides no support. English: competitive comparison with Nougat trained on millions of arXiv papers.}
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Chinese}} & \multicolumn{2}{c}{\textbf{English}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Edit$\downarrow$ & BLEU$\uparrow$ & Edit$\downarrow$ & BLEU$\uparrow$ \\
        \midrule
        MixTex-FT & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Nougat & \multicolumn{2}{c}{No support} & TBD & TBD \\
        \bottomrule
    \end{tabular}
    \label{tab:language}
\end{table}

\textbf{Key Findings:} (1) Fine-tuning on just 500 real samples provides significant improvement over pretraining alone (RQ1). (2) MixTex shows [TBD: describe domain adaptation results] across educational levels compared to Nougat (RQ2). (3) Artificially constructed training successfully enables Chinese LaTeX OCR where existing systems provide no support, while achieving competitive English performance compared to models trained on millions of arXiv papers (RQ3). Total training cost: 2 hours data generation + 16 hours pretraining + 30 minutes fine-tuning = $<$17 hours total, compared to weeks of manual data collection for traditional approaches.

\section{Discussion and Limitations}

\textbf{Practical Impact.} Our approach dramatically reduces the barrier to entry for LaTeX OCR development. Traditional approaches require weeks of manual data collection and cleaning—Nougat's training required millions of arXiv papers collected over extended periods. In contrast, our method generates 1M training samples in 2 hours with zero manual effort. The required fine-tuning dataset (500 samples, $\sim$15 hours collection) is feasible for individual researchers, compared to the institutional resources needed for arXiv-scale data collection.

\textbf{Domain Adaptation.} A key finding is that artificially constructed training data, lacking domain-specific text-formula correlations, appears to provide better generalization to out-of-distribution domains [TBD: pending experimental results]. This has practical implications: a single model trained on domain-agnostic data may serve multiple use cases (K--12 education, undergraduate textbooks, research papers) without requiring separate domain-specific training datasets.

\textbf{Intrinsic Multilingual Capability.} Unlike existing systems that require language-specific training data (e.g., Nougat trained on English arXiv papers can only process English), our approach has intrinsic multilingual capability. Since mathematical formulas are language-agnostic, we only need to substitute the text source to support a new language. Generating artificially constructed training data requires: (1) Wikipedia text in the target language (available for 300+ languages), and (2) mathematical formulas from im2latex (language-agnostic). This is not a generalization problem but a fundamental advantage: we can generate training data for any language without any real LaTeX documents in that language. For example, we enabled Chinese LaTeX OCR by simply using Chinese Wikipedia text, despite <1\% of Chinese academic papers providing LaTeX source files. This reduces the language extension problem from ``collect thousands of real documents'' (often impossible for low-resource languages) to ``collect 500 documents for fine-tuning''—a 1000$\times$ reduction in manual effort.

\textbf{Limitations.} (1) We have validated only on Chinese and English educational materials; extension to other languages and domains requires further evaluation. (2) The test set size (500 samples) is relatively modest, though it represents $\sim$30 hours of manual collection effort. (3) We fine-tune on 500 real samples; whether smaller fine-tuning sets (e.g., 100, 250) suffice is unexplored. (4) The optimal ratio of artificial to real training data remains an open question.

\textbf{Applicability.} This two-stage training paradigm (large-scale artificial pretraining + small-scale real fine-tuning) may apply to other document understanding tasks facing data scarcity, such as digitizing historical mathematical texts, handwritten scientific notes, or domain-specific technical documents.

\section{Conclusion}

We propose MixTex, a two-stage training approach for LaTeX OCR that addresses the data collection bottleneck through artificially constructed training data. By randomly pairing real natural language text from Wikipedia with mathematical formulas, we generate 1M training samples in approximately 2 hours with zero manual effort. After pretraining on this artificially constructed data, we fine-tune on only 500 real LaTeX documents (representing $\sim$15 hours of manual collection).

Evaluation on 500 independently collected real samples from educational materials (elementary school through university level, Chinese and English) demonstrates two key achievements: (1) competitive performance on English materials compared to Nougat trained on millions of arXiv papers, and (2) successful Chinese LaTeX OCR where existing systems like Nougat provide no support. Critically, our method shows [TBD: describe domain adaptation results] particularly on out-of-distribution educational materials where traditional arXiv-trained models struggle.

The practical impact is substantial: total development time (data generation + training) is under 17 hours on consumer hardware, compared to weeks or months of institutional effort for traditional data collection. For languages where LaTeX source files are scarce (e.g., Chinese, with <1\% of academic papers providing LaTeX sources), this approach enables LaTeX OCR development that would otherwise be impossible. The intrinsic multilingual capability—achieved by simply substituting Wikipedia text in the target language—makes this method applicable to 300+ languages without requiring any real LaTeX documents in those languages.

To facilitate reproducibility and future research, we will publicly release our code, model weights, data generation scripts, and the 500-sample test set. The 500-sample fine-tuning set will not be released due to copyright considerations, but researchers can replicate our approach by collecting their own domain-specific fine-tuning data.

Future work includes: (1) validation on additional languages beyond Chinese and English, (2) exploration of smaller fine-tuning set sizes (100--500 samples), (3) optimization of artificial-to-real data ratios, and (4) application to other document understanding tasks facing similar data scarcity challenges (historical texts, handwritten notes, domain-specific technical documents).

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
