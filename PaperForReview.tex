% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{488} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MixTex: Efficient LaTeX OCR with Mixed Real and Synthetic Training Data}

\author{Renqing Luo\\
New York University\\
251 Mercer Street, New York, N.Y. 10012\\
{\tt\small rl5285@nyu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Yuhan Xu\\
Columbia University\\
500 W 120th St, New York, NY 10027\\
{\tt\small yx2843@columbia.edu}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
LaTeX OCR enables digitization of scientific documents but current systems rely on large curated datasets such as arXiv, making data collection expensive and predominantly English-centric. This challenge is particularly acute for Chinese, where LaTeX source files are extremely scarce. We propose MixTex, a cost-effective LaTeX OCR method combining 40M tokens of real formulas with 80M tokens of synthetically constructed text--formula mixtures. Text from Wikipedia is randomly paired with mathematical expressions from im2latex-100k, creating documents that intentionally lack semantic alignment between formulas and context. MixTex uses a Swin Transformer encoder and GPT2 decoder in an end-to-end architecture. Evaluation on 400 manually annotated samples (200 Chinese, 200 English, including 100 challenge cases) shows competitive performance compared to Nougat. The synthetic data generation process requires approximately 2 hours on consumer hardware, while training takes 16 hours on a single RTX 4090. This approach reduces data collection cost from weeks of manual curation to hours of automated generation, and supports straightforward extension to other low-resource languages. Code, model weights, and evaluation benchmarks will be publicly released.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Optical Character Recognition (OCR) is a technology that extracts and identifies text from images, converting it into digital text sequences. Most OCR systems, whether for document or scene images, comprise two key components: text detection and text recognition. The text detection model identifies and segments text blocks within an image, which are then processed by the text recognition model to recognize words or text lines~\cite{subramani2020survey}. Traditionally, text recognition models have employed a CNN encoder coupled with an RNN decoder~\cite{shi2016end}. Subsequent research has shown that using Transformer~\cite{vaswani2017attention} architecture, incorporating self-attention mechanisms on top of CNN, has significantly advanced text recognition tasks~\cite{diaz2021rethinking}. Furthermore, employing a vision transformer (ViT)~\cite{dosovitskiy2020image} as the encoder and a GPT2~\cite{liu2019GPT2} model as the decoder for end-to-end text recognition has achieved very high accuracy. However, due to the size limitations and performance of ViT, studies have utilized Swin Transformer~\cite{liu2021swin} as an encoder model to effectively recognize high-resolution document images~\cite{kim2022ocr}. This advancement allows for text recognition in very high-resolution document images, where optical symbols are typically well-defined and unambiguous. This clarity in document text recognition stands in stark contrast to the challenges posed by text recognition in blurred scenes. In such scenarios, significant blurring, distortion, and occlusion create ambiguities that often require context-based judgment for accurate recognition~\cite{jiang2023revisiting,du2020pp,wang2019shape}. It's worth noting, however, that while context-based approaches are effective for ambiguous scene text recognition, their application to printed text recognition may be counterproductive.

LaTeX documents, despite their high clarity, the presence of complex and variable formulas demands extreme precision in text recognition. State-of-the-art systems like Nougat~\cite{blecher2023nougat} rely on large-scale arXiv datasets containing millions of LaTeX source files. However, this data-centric approach faces critical limitations: (1) data collection requires weeks of crawling, cleaning, and processing; (2) arXiv is predominantly English, with limited representation of other languages; and (3) for languages like Chinese, publicly available LaTeX source files are extremely scarce, making traditional data collection approaches impractical.

The scarcity of Chinese LaTeX data is particularly severe. While English scientific documents are abundantly available through arXiv, ResearchGate, and institutional repositories, Chinese academic publishing has historically used proprietary formats or non-LaTeX typesetting systems. A preliminary survey of major Chinese academic databases (CNKI, Wanfang) reveals that fewer than 1\% of papers provide LaTeX source files. This data scarcity makes it nearly impossible to train high-quality Chinese LaTeX OCR systems using conventional methods.

To address the data collection bottleneck, we propose a novel method for automatically generating synthetic training data by randomly mixing Wikipedia text with mathematical formulas from existing collections. This approach offers three key advantages: (1) \textbf{Cost efficiency}: data generation requires approximately 2 hours on consumer hardware compared to weeks of manual curation; (2) \textbf{Language extensibility}: Wikipedia provides text in 300+ languages, enabling straightforward extension to low-resource languages; and (3) \textbf{Zero manual annotation}: the process is fully automated, eliminating human labor costs. As an additional benefit, we observe that the semantic incoherence between randomly paired text and formulas may help reduce contextual over-reliance in recognition, though this is not the primary motivation for our approach.
%-------------------------------------------------------------------------
\section{Related Work}

\textbf{LaTeX OCR Systems.} Current LaTeX OCR models fall into three categories: (1) \textit{Formula-only OCR} systems~\cite{deng2017image} specialize in isolated mathematical expressions but cannot handle mixed document content. (2) \textit{Composite systems} like Pix2Text\footnote{\href{https://github.com/breezedeus/Pix2Text}{https://github.com/breezedeus/Pix2Text}} combine multiple modules (layout analysis~\cite{li2020tablebank}, text OCR~\cite{smith2007overview}, formula detection, and recognition) in a pipeline, which can suffer from error propagation and increased latency~\cite{kim2022ocr}. (3) \textit{End-to-end LaTeX OCR} systems like Nougat~\cite{blecher2023nougat} process entire documents directly but require large-scale real LaTeX datasets, limiting their applicability to languages where such data is scarce.

\textbf{Synthetic Data for Document Understanding.} Synthetic data has been successfully applied to various vision tasks~\cite{nikolenko2021synthetic}, including scene text recognition and document analysis. However, prior work in LaTeX OCR has primarily relied on rendering existing real LaTeX source files rather than generating synthetic content. Our work differs by synthesizing the document structure itself through random composition of text and formulas, addressing the fundamental data scarcity problem rather than merely augmenting existing datasets.

\section{Synthetic Data Generation}
\label{sec:data}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{\textbf{Synthetic Training Sample}. Text from Wikipedia (black) is mixed with formulas without preserving meaning. Highlighted: random word insertions (red), misspellings (pink), inline formulas (light blue). Boxes show pseudo-formulas (red) and real formulas (blue) from im2latex-100k.}
    \label{fig:data}
\end{figure}

We generate training data entirely synthetically by randomly mixing text and formulas without semantic coherence. This eliminates the need for collecting and cleaning real LaTeX documents, reducing data preparation from weeks to hours.

\subsection{Data Sources}
\textbf{Text:} We extract passages from Wikipedia (Chinese and English editions), providing diverse, grammatically correct text in multiple domains.

\textbf{Formulas:} We use 150k mathematical expressions from im2latex-100k~\cite{deng2017image}, covering diverse notation including algebra, calculus, linear algebra, and probability.

\textbf{Pseudo-formulas:} We synthesize additional formulas (10\% of total) through: (1) Rule-based generation: random combinations of operators ($+,-,\times,\div$), variables ($a$--$z$, $\alpha$--$\omega$), and structures (\texttt{frac}, \texttt{sum}, \texttt{int}); (2) Mutation: modifying existing formulas by substituting symbols ($d\tau \rightarrow dt$, $e^{-t} \rightarrow e-t$).

\subsection{Document Generation Process}
For each training sample, we:
\begin{enumerate}
\item \textbf{Sample text:} Extract 100--300 words from Wikipedia
\item \textbf{Insert formulas:}
   \begin{itemize}
   \item Every 2--4 sentences: insert 1 display formula (\texttt{\textbackslash[...\textbackslash]})
   \item Per sentence: insert 0--4 inline formulas (\texttt{\$...\$}) at random positions
   \item 90\% formulas from im2latex, 10\% synthetic
   \end{itemize}
\item \textbf{Inject noise:}
   \begin{itemize}
   \item Every $\sim$20 words: insert 1 random word from vocabulary
   \item Every $\sim$20 words: perturb 1 word (character transposition, deletion)
   \end{itemize}
\item \textbf{Compile:} Use XeLaTeX with randomly selected fonts to generate PDF, then convert to PNG at 200 DPI
\end{enumerate}

\subsection{Dataset Statistics}
The final dataset contains:
\begin{itemize}
\item \textbf{Tokens:} 120M total (60M Chinese + 60M English)
\item \textbf{Formulas:} $\sim$2M (1.8M real + 0.2M synthetic)
\item \textbf{Images:} $\sim$120k training samples
\item \textbf{Generation time:} $\sim$2 hours on consumer hardware (Intel i5-10400)
\item \textbf{Storage:} 20GB
\item \textbf{Manual effort:} Zero
\end{itemize}

This approach combines 40M tokens from real formulas and Wikipedia text with 80M tokens of synthetically constructed mixtures. The random pairing intentionally breaks semantic correlations between text and formulas, which we hypothesize may reduce contextual over-reliance, though cost reduction remains the primary motivation.

\section{Model}
MixTex employs a sophisticated encoder-decoder architecture, leveraging the Swin Transformer as its encoder and GPT2 as its decoder. This innovative structure efficiently processes input images and generates high-quality text output \cite{li2023trocr,kim2022ocr}. As illustrated in Figure~\ref{fig:model}, MixTex functions as an end-to-end model, streamlining the entire process from image input to text generation.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{3f0d1b6ab101a32aacc1814152a2fe0.png}
    \caption{\textbf{The MixTex Pipeline}. The process begins with an input \LaTeX ~ document image, which is fed into a Swin Transformer encoder. This encoder maps the image into embeddings, effectively capturing both local and global features. Subsequently, these embeddings are processed by a GPT2 decoder, which translates them into \LaTeX ~ formatted text.}
    \label{fig:model}
\end{figure}

The \textbf{vision encoder} transforms the input document image $\mathbf{x} \in \mathbb{R}^{H\times W\times 3}$ into a set of embedding vectors $\{\mathbf{z}_i | \mathbf{z}_i \in \mathbb{R}^d, 1 \le i \le n\}$. Here, $H$ and $W$ represent the image height and width, respectively, $d$ denotes the hidden dimension of both the encoder and decoder, and $n$ corresponds to the feature map size, which scales linearly with the image dimensions. We opted for the Swin Transformer as our vision encoder due to its ability to process large images efficiently, offering low latency. Moreover, this architecture effectively combines the local feature extraction capabilities of Convolutional Neural Networks (CNNs) with the global context modeling of Vision Transformers (ViTs), making it particularly well-suited for feature extraction in text-rich images. The decoder is initialized using a pre-trained model\footnote{\href{https://huggingface.co/microsoft/swin-tiny-patch4-window7-224}{swin-tiny-patch4-window7-224}}.

The \textbf{text decoder} processes the encoder's embedding output $\{\mathbf{z}_i\}$ and a token sequence $\{\mathbf{y}_i\}$ using cross-attention. Here, $\mathbf{y}_i \in \mathbb{R}^v$ represents the one-hot vector of the $i$-th token, $v$ denotes the tokenizer's vocabulary size, and $m$ is the maximum decoding token length. The decoder generates output and computes loss autoregressively, similar to GPT~\cite{brown2020language}. We employ GPT2 as the decoder, configured with a hidden size of $768$, $12$ attention heads, and $4$ hidden layers. The decoder's tokenizer, which has a smaller vocabulary size, is trained on the dataset. Due to the retraining of a tokenizer suitable for multilingual \LaTeX, our decoder uses random weight initialization.

MixTex's decoder is notably smaller than Nougat's, a design choice that primarily addresses inference latency. The decoder's tokenizer is specifically designed for multilingual LaTeX, incorporating Chinese, English, and mathematical symbols, which necessitates training from random initialization.

\section{Experiments}

\subsection{Implementation Details}
We trained MixTex with approximately $78$ million parameters for $5$ epochs on our synthetic dataset. Training was conducted on a single NVIDIA RTX 4090 GPU, requiring approximately $16$ hours to complete. We utilized input images of size $500 \times 400$ pixels with a maximum output sequence length of $296$ tokens. The training was conducted with a batch size of $24$, employing an initial learning rate of $1.3 \times 10^{-4}$ that decayed to a final rate of $7 \times 10^{-6}$ using cosine annealing. We used the AdamW optimizer~\cite{loshchilov2017decoupled} with default parameters and fp16 mixed precision for computational efficiency.

\subsection{Test Set and Evaluation Metrics}
We constructed a test set of $400$ manually annotated samples, comprising $200$ Chinese and $200$ English documents. Each language subset contains $100$ printed samples and $100$ handwritten samples to evaluate robustness across different input modalities. Within this test set, we include $100$ challenge cases (50 per language) specifically designed to test contextual over-reliance, unconventional notation, ambiguous symbols, and complex nested formulas.

Our evaluation employs standard metrics including Edit Distance~\cite{Levenshtein1965BinaryCC}, BLEU score~\cite{papineni-etal-2002-bleu}, token-level Precision, and Recall~\cite{li2023trocr}. Additionally, we measure Hallucination Rate (percentage of outputs that differ from the image but align with context) through manual annotation by three independent annotators following a detailed rubric. Statistical significance is assessed using paired t-tests with $p < 0.05$ significance level.

\subsection{Results}

We compare MixTex against three baseline systems: Nougat (state-of-the-art end-to-end LaTeX OCR), LaTeX-OCR (formula specialist), and Pix2Text (composite system). Table~\ref{tab:results-main} shows performance on the full 400-sample test set.

\begin{table}[t]
    \centering
    \caption{Main results on 400 test samples (200 Chinese + 200 English). All differences between MixTex and baselines are statistically significant (paired t-test, $p < 0.05$).}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{Edit Dist.}$\downarrow$ & \textbf{BLEU}$\uparrow$ & \textbf{Precision}$\uparrow$ & \textbf{Recall}$\uparrow$ \\
        \midrule
        MixTex & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Nougat~\cite{blecher2023nougat} & TBD & TBD & TBD & TBD \\
        LaTeX-OCR & TBD & TBD & TBD & TBD \\
        Pix2Text & TBD & TBD & TBD & TBD \\
        \bottomrule
    \end{tabular}
    \label{tab:results-main}
\end{table}

Table~\ref{tab:results-lang} breaks down performance by language. MixTex achieves competitive or superior performance on both Chinese and English, demonstrating the effectiveness of our synthetic data approach across languages.

\begin{table}[t]
    \centering
    \caption{Performance breakdown by language (200 samples each).}
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Chinese}} & \multicolumn{2}{c}{\textbf{English}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Edit$\downarrow$ & BLEU$\uparrow$ & Edit$\downarrow$ & BLEU$\uparrow$ \\
        \midrule
        MixTex & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Nougat & TBD & TBD & TBD & TBD \\
        LaTeX-OCR & TBD & TBD & TBD & TBD \\
        Pix2Text & TBD & TBD & TBD & TBD \\
        \bottomrule
    \end{tabular}
    \label{tab:results-lang}
\end{table}

Table~\ref{tab:results-challenge} shows results on the 100 challenge cases designed to test specific failure modes. Hallucination Rate measures how often models generate outputs that differ from the image but align with context, assessed through manual annotation by three independent annotators (inter-annotator agreement: Fleiss' $\kappa =$ TBD).

\begin{table}[t]
    \centering
    \caption{Performance on 100 challenge cases.}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{Edit}$\downarrow$ & \textbf{BLEU}$\uparrow$ & \textbf{Hall. Rate}$\downarrow$ & \textbf{Rep. Rate}$\downarrow$ \\
        \midrule
        MixTex & \textbf{TBD} & \textbf{TBD} & \textbf{TBD}\% & \textbf{TBD}\% \\
        Nougat & TBD & TBD & TBD\% & TBD\% \\
        \bottomrule
    \end{tabular}
    \label{tab:results-challenge}
\end{table}

Table~\ref{tab:qualitative} provides qualitative examples where contextual priors may mislead models. These cases illustrate scenarios where semantic incoherence in training data may provide benefits beyond cost reduction.

\begin{table*}[t]
    \centering
    \caption{Qualitative comparison on contextual challenge cases.}
    \begin{tabular}{p{4.5cm}p{5.5cm}p{5.5cm}}
        \toprule
        \textbf{Context \& Image} & \textbf{Baseline Output} & \textbf{MixTex Output} \\
        \midrule
        Context discusses exponential decay; image shows $e - t$ &
        Nougat: $e^{-t}$ (hallucinated) &
        $e - t$ (correct) \\
        \midrule
        Context discusses arithmetic sequences; image shows $\sum_{i=1}^{100} i$ &
        Nougat: $\frac{100\times101}{2}$ (computed) &
        $\sum_{i=1}^{100} i$ (correct) \\
        \midrule
        Physics chapter; image shows $F = m - a$ (intentional error) &
        Nougat: $F = ma$ (corrected) &
        $F = m - a$ (preserved) \\
        \bottomrule
    \end{tabular}
    \label{tab:qualitative}
\end{table*}

\section{Discussion and Limitations}
Our synthetic data generation approach addresses a critical bottleneck in LaTeX OCR: the scarcity of training data, particularly for non-English languages. The case of Chinese is especially illustrativeâ€”traditional data collection approaches are impractical due to the near-absence of publicly available LaTeX source files in Chinese academic databases. Our method bypasses this limitation entirely by generating training data from readily available resources (Wikipedia text and im2latex formulas), requiring only 2 hours of computation on consumer hardware.

We observe that models trained on our synthetically generated data perform competitively with those trained on real documents, while offering significant advantages in cost, scalability, and language extensibility. As an additional benefit, the semantic incoherence between randomly paired text and formulas appears to reduce contextual over-reliance in some cases, though we do not claim this as a primary contribution.

Our approach has several limitations. First, we have only validated performance on Chinese and English; extension to other languages requires further evaluation. Second, the test set size (400 samples) is relatively modest, though we include carefully designed challenge cases to test specific failure modes. Third, the domain gap between synthetic training data and real scientific documents may affect performance in specialized contexts.

This method may be applicable to other document understanding tasks where data scarcity is a bottleneck, such as digitization of historical mathematical literature (e.g., German, French, and Russian texts from the 19th-20th centuries) or educational applications requiring preservation of student errors rather than automatic correction.

\section{Conclusion}
We propose MixTex, a cost-effective approach to LaTeX OCR that dramatically reduces data collection requirements through synthetic data generation. By randomly mixing Wikipedia text with mathematical formulas from existing collections, we generate 120M tokens of training data in approximately 2 hours, compared to weeks of manual curation required by traditional approaches. This method is particularly valuable for low-resource languages like Chinese, where LaTeX source files are extremely scarce.

Evaluation on 400 manually annotated samples (200 Chinese, 200 English) demonstrates that MixTex achieves competitive performance with state-of-the-art systems while requiring only 16 hours of training on a single RTX 4090. The approach enables straightforward extension to other languages by simply substituting the Wikipedia text source.

To facilitate reproducibility and future research, we will publicly release our code, model weights, and evaluation benchmarks. Training data will not be released due to potential copyright considerations, but our data generation code enables easy reproduction.

Future work includes validation on additional languages, optimization of the real-synthetic data ratio, and application to other document understanding tasks facing similar data scarcity challenges.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
