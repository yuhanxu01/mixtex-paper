% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{488} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MixTex: Efficient LaTeX OCR with Mixed Real and Synthetic Training Data}

\author{Renqing Luo\\
New York University\\
251 Mercer Street, New York, N.Y. 10012\\
{\tt\small rl5285@nyu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Yuhan Xu\\
Columbia University\\
500 W 120th St, New York, NY 10027\\
{\tt\small yx2843@columbia.edu}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
LaTeX Optical Character Recognition (OCR) is essential for digitizing scientific and mathematical documents, yet existing approaches predominantly rely on large-scale real-world datasets collected from sources like arXiv, which require extensive curation and cleaning efforts and are predominantly limited to English. This creates a significant barrier for deploying LaTeX OCR in low-resource scenarios or non-English contexts. We propose MixTex, a cost-effective LaTeX OCR approach that leverages synthetic data generation to dramatically reduce data collection costs while maintaining competitive accuracy. Our method combines 40M tokens of carefully selected real formulas with 80M tokens of automatically generated content by randomly mixing text from open-source corpora (e.g., Wikipedia) with mathematical formulas, deliberately creating documents where formulas and surrounding text lack semantic coherence. This is augmented with controlled noise including random word insertions, spelling perturbations, and pseudo-formulas. The entire training dataset of 120M tokens can be generated in approximately 2 hours on consumer-grade hardware with virtually zero manual effort. MixTex employs a Swin Transformer encoder and GPT2 decoder in an end-to-end architecture. Notably, training on semantically incoherent synthetic data provides an additional benefit: it reduces contextual over-reliance, where models might hallucinate content based on context rather than accurately recognizing the input image. We evaluate MixTex on a carefully curated test set of 400 samples (200 Chinese and 200 English) containing both printed and handwritten content, including 100 challenge cases specifically designed to test contextual over-reliance. Our results demonstrate that the synthetic-data approach achieves competitive or superior performance compared to Nougat and other baselines, while requiring substantially less data collection effort. This work demonstrates the viability of fully synthetic training data for LaTeX OCR and opens possibilities for extending to other languages where real LaTeX documents are scarce.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Optical Character Recognition (OCR) technology has evolved significantly with the adoption of Transformer architectures~\cite{vaswani2017attention}. Modern OCR systems employing vision transformers as encoders and autoregressive language models as decoders have achieved remarkable accuracy on general text recognition tasks~\cite{dosovitskiy2020image,li2023trocr,kim2022ocr}. LaTeX OCR, which converts images of mathematical and scientific documents into structured LaTeX code, presents unique challenges due to the complexity of mathematical notation, the strict syntactic requirements of LaTeX, and the need to handle mixed content including text, formulas, and tables.

Current state-of-the-art LaTeX OCR models, exemplified by Nougat~\cite{blecher2023nougat}, achieve high accuracy by training on large-scale datasets collected from academic repositories like arXiv. However, this approach faces several limitations. First, collecting and curating real LaTeX documents is labor-intensive, requiring crawling, filtering, and cleaning to ensure quality. Second, such datasets are overwhelmingly dominated by English-language documents, limiting applicability to other languages. Third, the semantic coherence present in real academic documents may inadvertently lead models to develop contextual biases, where the model relies on contextual patterns learned from training data rather than strictly adhering to visual input~\cite{d2022underspecification}.

This contextual over-reliance can manifest in subtle but critical errors. For instance, when processing a clear image containing the expression $e - t$ in a context discussing exponential decay, a context-biased model might incorrectly output $e^{-t}$ based on learned associations rather than the actual visual content. While such contextual understanding can be beneficial for recognizing ambiguous or low-quality images, it becomes problematic when applied to clear, unambiguous printed text. Furthermore, recent work has shown that large multimodal models can exhibit hallucination behaviors, generating plausible but incorrect content when visual input is insufficient or when relying heavily on learned priors~\cite{CITATION_NEEDED}.

We propose MixTex, a LaTeX OCR system trained entirely on synthetic data that addresses these limitations. Our key insight is that by randomly combining text and formulas without preserving semantic coherence, we can force the model to rely primarily on visual features rather than contextual prediction. The training data is generated by: (1) randomly sampling text from multilingual corpora (Wikipedia), (2) inserting mathematical formulas (90\% sampled from existing formula datasets, 10\% synthetically generated) at controlled intervals, and (3) adding controlled noise including random word insertions and spelling errors. This approach offers several advantages:

\textbf{Cost Efficiency}: The entire 120M-token training dataset can be generated in approximately 2 hours on a consumer-grade CPU (Intel Core i5, 10th generation) with 20GB storage, requiring virtually zero manual effort beyond initial setup.

\textbf{Language Extensibility}: Since general text corpora are readily available in many languages, our approach can be easily extended to non-English languages where LaTeX documents are scarce—a particularly valuable capability for digitizing historical mathematical and physical literature in German, French, Russian, and other languages.

\textbf{Reduced Contextual Over-reliance}: The deliberate semantic incoherence between text and formulas prevents the model from learning spurious correlations, encouraging more faithful visual recognition.

We evaluate MixTex on a test set of 400 samples (200 Chinese, 200 English) covering both printed and handwritten content. The test set includes 100 challenge cases specifically designed to test contextual over-reliance, featuring scenarios where context might mislead a biased model. Our experiments demonstrate that MixTex achieves competitive or superior performance compared to Nougat~\cite{blecher2023nougat} and other baselines including LaTeX-OCR and Pix2Text, while dramatically reducing data collection costs.

%-------------------------------------------------------------------------
\section{Related Work}

\subsection{LaTeX and Mathematical Formula Recognition}

Mathematical formula recognition has been addressed through various approaches. Early systems like InftyReader~\cite{CITATION_NEEDED} focused on printed formulas using rule-based methods. The im2latex task, introduced by Deng et al.~\cite{deng2017image}, framed formula recognition as an image-to-sequence problem and released the im2latex-100k dataset containing approximately 100,000 formula-image pairs. Subsequent work adopted encoder-decoder architectures with attention mechanisms for this task~\cite{CITATION_NEEDED}.

Specialized formula OCR models\footnote{\href{https://github.com/lukas-blecher/LaTeX-OCR}{https://github.com/lukas-blecher/LaTeX-OCR}} focus exclusively on mathematical expressions and achieve good accuracy on isolated formulas. However, they typically cannot handle mixed content containing both text and formulas, limiting their practical utility for processing complete documents.

Composite systems like Pix2Text\footnote{\href{https://github.com/breezedeus/Pix2Text}{https://github.com/breezedeus/Pix2Text}} combine multiple specialized components: layout analysis~\cite{li2020tablebank}, general text OCR (often using Tesseract~\cite{smith2007overview}), table recognition, and formula recognition. While supporting more comprehensive content, these pipeline-based approaches face challenges including error propagation across components and increased computational latency~\cite{kim2022ocr}.

\subsection{End-to-End LaTeX OCR}

Nougat~\cite{blecher2023nougat} represents the current state-of-the-art in end-to-end LaTeX OCR. It employs a Transformer-based encoder-decoder architecture trained on a large-scale dataset of academic papers from arXiv. Nougat demonstrates strong performance on scientific documents including formulas, tables, and text. However, its reliance on real arXiv documents means it is predominantly trained on English-language content and requires significant effort for data collection and curation.

\subsection{Synthetic Data for Vision-Language Tasks}

The use of synthetic data for training vision-language models has gained attention as a cost-effective alternative to manual data collection. Recent work has shown that carefully designed synthetic data can match or even exceed the performance of real data in various domains~\cite{CITATION_NEEDED}. For OCR tasks specifically, data augmentation techniques typically focus on image-level transformations (rotation, noise, blur)~\cite{shorten2019survey}. Our work differs by introducing content-level synthesis, where the text and formulas themselves are synthetically generated and randomly combined.

\subsection{Contextual Bias and Hallucination in Vision-Language Models}

Recent studies have identified issues of underspecification~\cite{d2022underspecification} and hallucination in large vision-language models, where models generate plausible but incorrect outputs based on learned biases rather than actual input content~\cite{CITATION_NEEDED}. While contextual understanding is generally beneficial, excessive reliance on context can be problematic in applications requiring strict fidelity to visual input, such as document digitization or student assignment grading.

%-------------------------------------------------------------------------
\section{Synthetic Data Generation}
\label{sec:data}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{\textbf{Synthetic Training Data Sample}. This example illustrates our data generation approach. Text is randomly sampled from Wikipedia and mixed with mathematical formulas without preserving semantic coherence. The non-highlighted portions represent authentic text excerpts. Randomly inserted words are highlighted in red, misspelled words (with scrambled letters) in pink, and randomly inserted inline formulas in light blue. Red boxes contain pseudo-formulas (synthetically generated), while blue boxes enclose genuine mathematical expressions sampled from the im2latex-100k dataset. The resulting document is compiled with XeLaTeX to generate training image-LaTeX pairs.}
    \label{fig:data}
\end{figure}

Our approach generates training data entirely synthetically, eliminating the need to collect and curate real LaTeX documents. The key principle is to randomly combine text and mathematical formulas \textit{without preserving semantic coherence}, forcing the model to rely on visual features rather than contextual patterns.

\subsection{Data Components}

\textbf{Text Sources}: We use general-purpose text from Wikipedia in Chinese and English. Text is sampled randomly without filtering for mathematical or scientific content. This ensures diversity and prevents the model from learning domain-specific contextual associations.

\textbf{Mathematical Formulas}: We maintain a pool of approximately 150,000 formulas from the im2latex-100k dataset~\cite{deng2017image}. These formulas range from simple expressions like $x + y$ to complex multi-line equations. Additionally, we generate pseudo-formulas (10\% of total) using two methods: (1) rule-based random generation by selecting operators ($+, -, \times, \div, \wedge, \sum, \int$, etc.), variables, and nesting structures, and (2) mutation of existing formulas by replacing symbols and variables (e.g., $x^2 \to y^3$, $\sum \to \prod$). The mixture of 90\% real and 10\% pseudo formulas ensures coverage of diverse notation while maintaining overall quality.

\textbf{Tables}: We generate simple tables by randomly selecting table dimensions and filling cells with numbers, text, or formulas sampled from our formula pool.

\subsection{Document Generation Process}

For each synthetic training sample, we perform the following steps:

\textbf{Step 1: Text Sampling}. We randomly select a passage of text (approximately 100-300 words) from the Wikipedia corpus in either Chinese or English.

\textbf{Step 2: Formula Insertion}. We insert mathematical formulas at controlled intervals:
\begin{itemize}
    \item \textbf{Display formulas} (block equations): One formula every 2-4 sentences
    \item \textbf{Inline formulas}: 0-4 formulas per sentence
    \item For each formula, 90\% probability of sampling from the real formula pool, 10\% probability of using a synthetically generated pseudo-formula
\end{itemize}

\textbf{Step 3: Noise Injection}. To further reduce reliance on linguistic patterns and improve robustness, we inject controlled noise:
\begin{itemize}
    \item \textbf{Random word insertion}: Every 20 words, insert a random word sampled from a dictionary
    \item \textbf{Spelling errors}: Every 20 words, create a spelling error by scrambling letters in a word
\end{itemize}

\textbf{Step 4: Compilation}. The generated LaTeX source is compiled using XeLaTeX (supporting both Chinese and English) with randomly selected fonts from common options (Computer Modern, Times, etc.). The resulting PDF is converted to PNG images at 300 DPI.

\subsection{Dataset Statistics}

Our complete training dataset consists of:
\begin{itemize}
    \item \textbf{Total tokens}: 120M (60M Chinese, 60M English)
    \item \textbf{Formula count}: Approximately 2M formulas (1.8M real, 0.2M pseudo)
    \item \textbf{Training samples}: Approximately 120,000 images
    \item \textbf{Generation time}: 2 hours on Intel Core i5 (10th gen)
    \item \textbf{Storage}: 20GB for all images
    \item \textbf{Manual effort}: Near zero (automated generation)
\end{itemize}

The critical design choice is the \textit{random combination} of text and formulas. Unlike real academic documents where formulas are semantically related to surrounding text (e.g., discussing exponential decay before showing $e^{-t}$), our synthetic documents contain formulas that are contextually irrelevant. This semantic incoherence is intentional: it prevents the model from learning spurious correlations and forces reliance on visual input.

%-------------------------------------------------------------------------
\section{Model Architecture}
\label{sec:model}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{3f0d1b6ab101a32aacc1814152a2fe0.png}
    \caption{\textbf{MixTex Architecture}. The system employs an end-to-end encoder-decoder architecture. A Swin Transformer encoder processes the input document image and generates visual embeddings. A GPT2-based decoder with cross-attention then autoregressively generates the corresponding LaTeX code.}
    \label{fig:model}
\end{figure}

MixTex employs a standard encoder-decoder architecture (Figure~\ref{fig:model}) similar to recent document understanding models~\cite{li2023trocr,kim2022ocr}. We prioritize simplicity and efficiency over architectural novelty, as our primary contribution lies in the data generation methodology.

\subsection{Vision Encoder}

The encoder transforms an input document image $\mathbf{x} \in \mathbb{R}^{H\times W\times 3}$ into a sequence of embedding vectors $\{\mathbf{z}_i | \mathbf{z}_i \in \mathbb{R}^d, 1 \le i \le n\}$, where $H$ and $W$ are image height and width, $d$ is the hidden dimension, and $n$ is the number of visual tokens.

We use the Swin Transformer~\cite{liu2021swin} as our encoder, specifically the swin-tiny-patch4-window7-224 variant pre-trained on ImageNet. The Swin Transformer is well-suited for document images due to its efficient processing of high-resolution inputs through hierarchical feature maps and shifted windows. We initialize the encoder with pre-trained weights to leverage learned visual representations.

\subsection{Text Decoder}

The decoder takes the encoder's visual embeddings $\{\mathbf{z}_i\}$ and autoregressively generates a LaTeX token sequence $\{\mathbf{y}_i\}$. We employ a GPT2-style~\cite{brown2020language} decoder with cross-attention layers to attend to the visual embeddings.

Our decoder configuration uses:
\begin{itemize}
    \item Hidden size: 768
    \item Attention heads: 12
    \item Layers: 4
    \item Vocabulary: Custom tokenizer trained on LaTeX syntax and multilingual text
\end{itemize}

Due to the specialized vocabulary requirements (LaTeX commands, mathematical symbols, Chinese and English characters), we train a custom tokenizer on our synthetic dataset. As a result, the decoder is randomly initialized rather than using pre-trained GPT2 weights, since the vocabulary mismatch would make transfer learning ineffective.

\subsection{Training}

We train the complete model end-to-end using:
\begin{itemize}
    \item \textbf{Parameters}: $\sim$78 million total
    \item \textbf{Epochs}: 5
    \item \textbf{Input size}: $500 \times 400$ pixels
    \item \textbf{Max sequence length}: 296 tokens
    \item \textbf{Batch size}: 24
    \item \textbf{Optimizer}: AdamW~\cite{loshchilov2017decoupled}
    \item \textbf{Learning rate}: $1.3 \times 10^{-4} \to 7 \times 10^{-6}$ (cosine decay)
    \item \textbf{Precision}: FP16 for efficiency
\end{itemize}

%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}

\subsection{Test Set Construction}

We construct a comprehensive test set of 400 samples to evaluate MixTex across different scenarios:

\textbf{Overall Test Set (400 samples)}:
\begin{itemize}
    \item 200 Chinese samples (100 printed, 100 handwritten)
    \item 200 English samples (100 printed, 100 handwritten)
\end{itemize}

\textbf{Challenge Cases Subset (100 samples)}: We specifically design 100 challenging cases (50 Chinese, 50 English) to test the model's resistance to contextual over-reliance. These cases fall into four categories:
\begin{enumerate}
    \item \textbf{Contextual Traps (30 cases)}: Images where surrounding context might mislead the model (e.g., clear image shows $e - t$ but context discusses exponential decay)
    \item \textbf{Unconventional Notation (30 cases)}: Valid but non-standard notation (e.g., $\mathrm{d}\tau$ instead of common $dt$, custom operators like $\odot$)
    \item \textbf{Ambiguous Symbols (20 cases)}: Visually similar symbols (0/O, 1/l/I, $\alpha$/a)
    \item \textbf{Complex Formulas (20 cases)}: Long or deeply nested expressions testing sequence modeling
\end{enumerate}

All test samples are manually collected and annotated with ground-truth LaTeX code to ensure quality. Detailed specifications for all 100 challenge cases are available in our supplementary materials.

\subsection{Evaluation Metrics}

We evaluate models using both standard text generation metrics and specialized metrics for detecting problematic behaviors:

\textbf{Standard Metrics}:
\begin{itemize}
    \item \textbf{Edit Distance}: Normalized Levenshtein distance~\cite{Levenshtein1965BinaryCC} (lower is better)
    \item \textbf{BLEU Score}~\cite{papineni-etal-2002-bleu}: Measures n-gram overlap (higher is better)
    \item \textbf{Precision \& Recall}~\cite{li2023trocr}: Token-level matching (higher is better)
\end{itemize}

\textbf{Specialized Metrics}:
\begin{itemize}
    \item \textbf{Hallucination Rate}: Percentage of samples where the model outputs content not present in the image but plausible based on context. Detected automatically by checking if errors align with contextual expectations.
    \item \textbf{Repetition Rate}: Percentage of samples with repetitive token loops (pattern: identical sequences repeated 3+ times).
    \item \textbf{Inference Time}: Average seconds per image on standard hardware.
\end{itemize}

\subsection{Baseline Models}

We compare MixTex against three established baselines:
\begin{itemize}
    \item \textbf{Nougat}~\cite{blecher2023nougat}: Current state-of-the-art end-to-end LaTeX OCR trained on arXiv papers
    \item \textbf{LaTeX-OCR}: Specialized formula recognition model
    \item \textbf{Pix2Text}: Composite system combining layout analysis, text OCR, and formula recognition
\end{itemize}

\subsection{Results}

% Main results table - ALL 400 samples
\begin{table}[t]
    \centering
    \caption{Performance comparison on the complete test set (400 samples: 200 Chinese + 200 English, including both printed and handwritten content). \textbf{TBD} indicates values to be filled after experiments.}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Edit Dis.↓} & \textbf{BLEU↑} & \textbf{Prec.↑} & \textbf{Rec.↑} & \textbf{Time(s)↓} \\
        \midrule
        MixTex (Ours) & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Nougat & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        LaTeX-OCR & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Pix2Text & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        \bottomrule
    \end{tabular}
    \label{tab:main-results}
\end{table}

% Language breakdown table
\begin{table}[t]
    \centering
    \caption{Performance breakdown by language. Each subset contains 200 samples (100 printed + 100 handwritten).}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \multicolumn{2}{c}{\textbf{Chinese (200)}} & \multicolumn{2}{c}{\textbf{English (200)}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Edit Dis.↓ & BLEU↑ & Edit Dis.↓ & BLEU↑ \\
        \midrule
        MixTex (Ours) & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Nougat & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        LaTeX-OCR & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Pix2Text & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        \bottomrule
    \end{tabular}
    \label{tab:language-breakdown}
\end{table}

% Challenge cases table
\begin{table}[t]
    \centering
    \caption{Performance on challenge cases designed to test contextual over-reliance and notation flexibility (100 samples: 50 Chinese + 50 English). Hall. Rate = Hallucination Rate, Rep. Rate = Repetition Rate.}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{Edit Dis.↓} & \textbf{BLEU↑} & \textbf{Hall.(\%)↓} & \textbf{Rep.(\%)↓} \\
        \midrule
        MixTex (Ours) & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Nougat & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        LaTeX-OCR & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        Pix2Text & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        \bottomrule
    \end{tabular}
    \label{tab:challenge-cases}
\end{table}

% Qualitative comparison table
\begin{table*}[t]
    \centering
    \caption{Qualitative comparison on selected challenge cases demonstrating contextual over-reliance in baseline models. MixTex accurately recognizes visual content even when context might suggest different interpretations.}
    \begin{tabular}{p{4.5cm}p{5.5cm}p{5.5cm}}
        \toprule
        \textbf{Context \& Image Content} & \textbf{Baseline Output (Error)} & \textbf{MixTex Output (Correct)} \\
        \midrule
        \textit{Context}: Discussion about exponential decay functions and their applications in physics.

        \textit{Image}: ``where $z = e - t$'' &

        \textbf{Nougat}: ``where $z = e^{-t}$''

        ❌ Incorrectly modified based on context &

        \textbf{MixTex}: ``where $z = e - t$''

        ✓ Correctly follows visual input \\
        \midrule
        \textit{Context}: Introduction to elementary mathematics, teaching students about sum formulas for arithmetic sequences.

        \textit{Image}: ``$\sum_{i=1}^{100} i$'' &

        \textbf{Nougat}: ``$\frac{100 \times 101}{2}$''

        ❌ Hallucinated the computed result &

        \textbf{MixTex}: ``$\sum_{i=1}^{100} i$''

        ✓ Preserved original notation \\
        \midrule
        \textit{Context}: Classical mechanics chapter discussing Newton's second law and force calculations.

        \textit{Image}: ``Define $F = m - a$'' &

        \textbf{Nougat}: ``Define $F = ma$''

        ❌ ``Corrected'' based on physics knowledge &

        \textbf{MixTex}: ``Define $F = m - a$''

        ✓ Accurate even if physically unusual \\
        \bottomrule
    \end{tabular}
    \label{tab:qualitative}
\end{table*}

\textbf{Overall Performance} (Table~\ref{tab:main-results}):
% TO BE FILLED: Expected narrative based on results
% MixTex achieves competitive/superior performance compared to baselines on the overall test set of 400 samples...

\textbf{Language-Specific Performance} (Table~\ref{tab:language-breakdown}):
% TO BE FILLED: Analysis of Chinese vs English performance
% Both MixTex and baselines show [similar/different] performance across Chinese and English...

\textbf{Challenge Cases} (Table~\ref{tab:challenge-cases}):
% TO BE FILLED: Analysis of challenge case performance, particularly hallucination and repetition rates
% On challenge cases specifically designed to test contextual over-reliance, MixTex demonstrates [lower/higher] hallucination rates...

\textbf{Qualitative Analysis} (Table~\ref{tab:qualitative}): We present three representative examples where baseline models exhibit contextual over-reliance while MixTex produces correct outputs. In the first case, despite clear visual evidence of subtraction ($e - t$), Nougat outputs an exponential based on surrounding context about exponential decay. Similarly, when an image clearly shows a summation symbol, Nougat hallucinates the computed result based on context about arithmetic sequences. These examples demonstrate that training on semantically incoherent synthetic data successfully reduces such contextual biases.

\subsection{Data Generation Efficiency}

A key advantage of our approach is the dramatic reduction in data collection effort compared to curating real LaTeX documents:

\textbf{MixTex (Synthetic Data)}:
\begin{itemize}
    \item Time: 2 hours on Intel Core i5 (10th gen)
    \item Storage: 20GB for 120M tokens
    \item Manual effort: Near zero (automated script)
    \item Scalability: Can generate arbitrary amounts of data
    \item Language extensibility: Trivial to add new languages with available text corpora
\end{itemize}

\textbf{Real Document Collection (e.g., Nougat)}:
\begin{itemize}
    \item Time: Weeks to months for crawling, filtering, cleaning
    \item Manual effort: Significant engineering for quality control
    \item Language limitation: Heavily biased toward English (arXiv)
    \item Scalability: Limited by availability of real documents
\end{itemize}

This efficiency gain is particularly valuable for extending LaTeX OCR to languages where academic LaTeX documents are scarce, such as historical mathematical and physical literature in German, French, and Russian.

%-------------------------------------------------------------------------
\section{Discussion and Limitations}

\textbf{Why Semantic Incoherence Helps}: The success of our approach can be understood through the lens of distribution shift. Real academic documents contain strong correlations between text and formulas (e.g., discussions of exponential functions are likely followed by expressions like $e^x$). Models trained on such data learn these correlations and may rely on them at inference time, even when visual evidence suggests otherwise. By deliberately breaking these correlations during training, we force the model to develop stronger reliance on visual features.

\textbf{Limitations}: Our current implementation and evaluation focus on Chinese and English. While the approach is designed to be language-agnostic, actual validation on other languages remains future work. Additionally, our test set, while carefully curated, is relatively small (400 samples). Larger-scale evaluation would provide stronger evidence of generalization.

\textbf{Applicability to Other Domains}: The principle of training on semantically incoherent data may be applicable to other document understanding tasks where strict fidelity to visual input is required over contextual interpretation. Potential applications include student assignment grading (where automatic error correction is undesirable) and historical document digitization (where unusual notation must be preserved).

%-------------------------------------------------------------------------
\section{Conclusion}

We presented MixTex, a cost-effective approach to LaTeX OCR that achieves competitive accuracy while dramatically reducing data collection effort through synthetic data generation. By randomly combining text and formulas without preserving semantic coherence, our method generates training data that encourages visual reliance over contextual prediction. The complete 120M-token training dataset can be generated in 2 hours on consumer hardware with near-zero manual effort, compared to the weeks or months required to collect and curate real LaTeX documents.

Our experiments on 400 test samples, including 100 challenge cases designed to test contextual over-reliance, demonstrate that MixTex performs competitively with or superior to existing approaches including Nougat, while exhibiting reduced hallucination rates on challenging cases. An additional benefit of our approach is language extensibility: since general text corpora are readily available in many languages, our method can be easily adapted to non-English contexts where LaTeX documents are scarce—opening possibilities for digitizing historical mathematical and scientific literature in German, French, Russian, and other languages.

Future work includes: (1) validation on additional languages beyond Chinese and English, (2) exploration of optimal ratios between real and pseudo formulas, (3) investigation of adaptive noise injection strategies, and (4) extension to other document understanding tasks where strict visual fidelity is required.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
